{"cells":[{"cell_type":"markdown","metadata":{"id":"xd5ZI3FfFku2"},"source":["# References: https://github.com/youbeebee/deeplearning_from_scratch"]},{"cell_type":"markdown","metadata":{"id":"2WRoeHk2BRLa"},"source":["수강생분의 이름, 학번을 반영해주세요."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1666669353593,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"p6lZMos0BRLb","outputId":"bd5b6e17-1666-4627-a6e2-66ab5560cb70"},"outputs":[{"output_type":"stream","name":"stdout","text":["20201944 안희진\n"]}],"source":["id = '20201944'\n","name = '안희진'\n","print(id, name)"]},{"cell_type":"markdown","metadata":{"id":"MBj1kBpvEjy4"},"source":["구글 드라이브 연동"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30377,"status":"ok","timestamp":1666669383964,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"zPGWitzhEjl_","outputId":"82e1e98b-d26d-436c-81c4-33d6dba18b95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"Rz9zrCwHE7Y9"},"source":["폴더 경로 설정"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666669383965,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"sfZV5eWRE7oG"},"outputs":[],"source":["workspace_path = '/gdrive/My Drive/3-2/인공지능/Al_week8/codes'  # 과제 파일 업로드한 경로 반영"]},{"cell_type":"markdown","metadata":{"id":"mYXRFUrREx63"},"source":["폴더 접근 허용"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1666669383965,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"NhuEyESNEyL3"},"outputs":[],"source":["import sys\n","sys.path.append(workspace_path)"]},{"cell_type":"markdown","metadata":{"id":"5zllflSzHh_z"},"source":["실험결과 재현 함수"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1666669383965,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"5csm7Qh1HiX0"},"outputs":[],"source":["import numpy as np\n","import random\n","\n","def seed_everything(seed):\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    # torch.manual_seed(seed)\n","    # if torch.cuda.is_available():\n","    #     torch.cuda.manual_seed(seed)\n","    #     if torch.cuda.device_count() > 1:\n","    #         torch.cuda.manual_seed_all(seed) # if use multi-GPU\n","    #     torch.backends.cudnn.deterministic = True\n","    #     torch.backends.cudnn.benchmark = False"]},{"cell_type":"markdown","metadata":{"id":"ahW_aqPWF3yx"},"source":["# 신경망 학습"]},{"cell_type":"markdown","metadata":{"id":"A24rzsRaNf4G"},"source":["손실함수: 평균제곱오차, 교차 엔트로피 오차"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1666669383966,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"f35aBUozTot0","outputId":"f09dfc91-faf8-4f40-9690-9c2e845b690d"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.09750000000000003\n","0.5975\n","0.510825457099338\n","2.3025840929945454\n"]}],"source":["import numpy as np\n","'''\n","손실 함수loss function : 신경망 성능의 '나쁨'을 나타내는 지표.\n","일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용\n","'''\n","\n","\n","# 4.2.1 평균 제곱 오차\n","# E = 1/2 * ∑ _k (yk-tk)²\n","# yk : 신경망의 출력\n","# tk : 정답 레이블\n","# k : 데이터의 차원 수\n","def mean_squared_error(y, t):\n","    return 0.5 * np.sum((y-t)**2)\n","\n","\n","# 정답은 '2'\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] #정답값\n","\n","# ex1 '2'일 확률이 가장 높다고 추정함(0.6)\n","y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","mse = mean_squared_error(np.array(y), np.array(t))\n","print(mse)  # 0.0975\n","# ex2 '7'일 확률이 가장 높다고 추정함(0.6)\n","y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","mse = mean_squared_error(np.array(y), np.array(t))\n","print(mse)  # 0.5975\n","\n","\n","# 4.2.2 교차 엔트로피 오차\n","# E = -∑ _k (tk * log(yk))\n","# log : 자연로그\n","# yk : 신경망의 출력\n","# tk : 정답 레이블(one-hot encoding)\n","# k : 데이터의 차원 수\n","# 실질적으로 정답일때의 추정의 자연로그를 계산하는 식이 됨\n","def cross_entropy_error(y, t):\n","    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n","    return -np.sum(t * np.log(y + delta))\n","\n","\n","# 동일한 계산\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] #정답이이 맞을떄\n","y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","cee = cross_entropy_error(np.array(y), np.array(t))\n","print(cee)  # 0.510825457099\n","y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","cee = cross_entropy_error(np.array(y), np.array(t))\n","print(cee)  # 2.30258409299\n"]},{"cell_type":"markdown","metadata":{"id":"HhzdIlHKi94z"},"source":["미니배치 교차 엔트로피 오차"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6568,"status":"ok","timestamp":1666669390528,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"auIqI4SRi9nz","outputId":"8c752d49-093a-4dc3-e0f8-dfa37176c705"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 784)\n","(60000,)\n"]}],"source":["import numpy as np\n","import sys\n","import os\n","sys.path.append(os.pardir)\n","from dataset.mnist import load_mnist\n","\n","# 4.2.3 미니배치 학습\n","# 훈련 데이터 전체에 대한 오차함수\n","# E = -1/N * ∑ _n (∑ _k (tk * log(yk)))\n","# N : 데이터의 개수\n","# 훈련 데이터 전체에 대한 손실 함수를 계산하기에는 시간이 오래걸리기 때문에\n","# 일부를 추려 전체의 근사치로 이용할 수 있다.\n","(x_train, t_train), (x_test, t_test) = \\\n","    load_mnist(normalize=True, one_hot_label=False)\n","\n","print(x_train.shape)  # (60000, 784)\n","print(t_train.shape)  # 원-핫 인코딩 된 정답 레이블 (60000, 10)\n","\n","# 무작위 10개 추출\n","train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size)\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]\n","\n","\n","# 4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","\n","    batch_size = y.shape[0]\n","    return -np.sum(t * np.log(y[np.arange(batch_size), t])) / batch_size\n","\n","\n","# 4.2.5 왜 손실 함수를 설정하는가?\n","# 신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다.\n","# 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.\n","# (매개변수의 미소한 변화에는 거의 반응을 보이지 않고 그 값이 분연속적으로 변화)\n"]},{"cell_type":"markdown","metadata":{"id":"EmvpQMRoFTLC"},"source":["수치 미분"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1666669390528,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"x1nD-RqBFTcb","outputId":"27d17ced-8d00-4049-c55f-84f2a8c3371a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.200000000000089\n","0.29999999999996696\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["5.999999999998451\n","8.000000000000895\n"]}],"source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","\n","# 4.3.1 미분\n","# 나쁜 구현 예\n","def numerical_diff_bad(f, x):\n","    h = 10e-50\n","    return (f(x + h) - f(x)) / h\n","# h값이 너무 작아 반올림 오차를 일으킬 수 있음 10e-4정도가 적당하다고 알려짐\n","# 전방 차분에서는 차분이 0이 될 수 없어 오차가 발생\n","#  -> 오차를 줄이기 위해 중심 차분을 사용\n","\n","\n","def numerical_diff(f, x):\n","    h = 10e-4\n","    return (f(x + h) - f(x - h)) / (2 * h)\n","\n","\n","# 4.3.2 수치 미분의 예\n","# y = 0.01x² + 0.1x\n","def function_1(x):\n","    return 0.01*x**2 + 0.1*x\n","\n","\n","x = np.arange(0.0, 20.0, 0.1)  # 0에서 20까지 간격 0.1인 배열 x를 만든다.\n","y = function_1(x)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"f(x)\")\n","plt.plot(x, y)\n","# plt.show()\n","\n","# x = 5, 10일때 미분\n","print(numerical_diff(function_1, 5))   # 0.200000000000089\n","print(numerical_diff(function_1, 10))  # 0.29999999999996696\n","\n","\n","# 접선의 함수를 구하는 함수\n","def tangent_line(f, x):\n","        d = numerical_diff(f, x)\n","        # print(d)\n","        y = f(x) - d*x\n","        return lambda t: d*t + y\n","\n","\n","tf = tangent_line(function_1, 5)\n","y2 = tf(x)\n","plt.plot(x, y2)\n","plt.show()\n","\n","\n","# 4.3.3 편미분\n","# f(x0, x1) = x0² + x1²\n","def function_2(x):\n","    return x[0]**2 + x[1]**2\n","    # or return np.sum(x**2)\n","\n","\n","# x0 = 3, x1 = 4일 때, x0에 대한 편미분을 구하라.\n","def function_tmp1(x0):\n","    return x0**2 + 4.0**2.0\n","\n","\n","# x0 = 3, x1 = 4일 때, x1에 대한 편미분을 구하라.\n","def function_tmp2(x1):\n","    return 3.0**2.0 + x1 * x1\n","\n","\n","print(numerical_diff(function_tmp1, 3.0))  # 5.999999999998451\n","print(numerical_diff(function_tmp2, 4.0))  # 8.000000000000895\n"]},{"cell_type":"markdown","metadata":{"id":"asO7sXybFXEz"},"source":["기울기 계산"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1666669390528,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"FnovbeF3FXUT","outputId":"d358bd30-2433-4759-ce48-f023074dee98"},"outputs":[{"output_type":"stream","name":"stdout","text":["[6. 8.]\n","[0. 4.]\n","[6. 0.]\n","[-6.11110793e-10  8.14814391e-10]\n","[-0.39785867  0.53047822]\n","[-2.45570041  3.27426722]\n","[-2.58983747e+13 -1.29524862e+12]\n","[-2.99999994  3.99999992]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","\n","# 앞 절에서 x0, x1에 대한 편미분을 변수별로 따로 계산했음.\n","# x0, x1의 편미분을 동시에 계산하고 싶다면?\n","# x0 = 3, x1 = 4일 때 (x0, x1) 양쪽의 편미분을 묶어 벡터로 정리한 것을 기울기gradient라고 한다.\n","def numerical_gradient(f, x):\n","    h = 1e-4\n","    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성\n","\n","    for idx in range(x.size):\n","        tmp_val = x[idx]\n","        # f(x+h) 계산\n","        x[idx] = tmp_val + h\n","        fxh1 = f(x)\n","\n","        # f(x-h) 계산\n","        x[idx] = tmp_val - h\n","        fxh2 = f(x)\n","\n","        grad[idx] = (fxh1 - fxh2) / (2 * h)\n","        x[idx] = tmp_val  # 값 복원\n","\n","    return grad\n","\n","\n","# f(x0, x1) = x0² + x1²\n","def function_2(x):\n","    return x[0]**2 + x[1]**2\n","    # or return np.sum(x**2)\n","\n","\n","print(numerical_gradient(function_2, np.array([3.0, 4.0])))  # [ 6.  8.]\n","print(numerical_gradient(function_2, np.array([0.0, 2.0])))  # [ 0.  4.]\n","print(numerical_gradient(function_2, np.array([3.0, 0.0])))  # [ 6.  0.]\n","\n","# 4.4.1 경사법(경사 하강법)\n","# x0 = x0 - η*∂f/∂x0\n","# x1 = x1 - η*∂f/∂x1\n","# η(eta) : 갱신하는 양, 학습률learning rate\n","# 위 식을 반복\n","\n","\n","# f:최적화하려는 함수\n","# init_x : 초깃값\n","# lr : 학습률\n","# step_num : 반복횟수\n","def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","    x = init_x\n","    x_history = []\n","\n","    for i in range(step_num):\n","        x_history.append(x.copy())\n","        grad = numerical_gradient(f, x)\n","        x -= lr * grad #손실값값 줄여줌줌\n","\n","\n","    return x, np.array(x_history)\n","\n","\n","# 경사법으로 f(x0, x1) = x0² + x1²의 최솟값을 구해라\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=0.1)\n","print(x)  # [ -6.11110793e-10   8.14814391e-10]\n","\n","# 경사법으로 f(x0, x1) = x0² + x1²의 최솟값을 구해라\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=0.01)\n","print(x)  # [ -6.11110793e-10   8.14814391e-10]\n","\n","# 경사법으로 f(x0, x1) = x0² + x1²의 최솟값을 구해라\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=0.001)\n","print(x)  # [ -6.11110793e-10   8.14814391e-10]\n","\n","# 학습률이 너무 큼\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=10.0)\n","print(x)  # [ -2.58983747e+13  -1.29524862e+12] 발산함\n","\n","# 학습률이 너무 작음\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=1e-10)\n","print(x)  # [-2.99999994  3.99999992] 거의 변화 없음\n","\n","# 그래프\n","init_x = np.array([-3.0, 4.0])\n","x, x_history = gradient_descent(function_2, init_x, lr=0.1, step_num=20)\n","\n","plt.plot([-5, 5], [0, 0], '--b')\n","plt.plot([0, 0], [-5, 5], '--b')\n","plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n","\n","plt.xlim(-3.5, 3.5)\n","plt.ylim(-4.5, 4.5)\n","plt.xlabel(\"X0\")\n","plt.ylabel(\"X1\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"BJxRcf5JFvUS"},"source":["학습 알고리즘 구현"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129825,"status":"ok","timestamp":1666669520346,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"7PYfsrL4Fvi6","outputId":"4309f1c7-0a67-4364-e2c9-6ced0f9540b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["(784, 100)\n","(100,)\n","(100, 10)\n","(10,)\n","(784, 100)\n","(100,)\n","(100, 10)\n","(10,)\n"]}],"source":["'''\n","전제\n","신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n","신경망 학습은 다음과 같이 4단계로 수행한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n","그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n","\n","2단계 - 기울기 산출\n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신\n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복한다.\n","\n","데이터를 무작위로 선정하기 때문에 확률적 경사 하강법stochastic gradient descent,\n","SGD라고 부른다.\n","'''\n","import os\n","import numpy as np\n","from common.functions import sigmoid, softmax, cross_entropy_error\n","from common.gradient import numerical_gradient\n","\n","\n","class TwoLayerNet:\n","    \"\"\"\n","    params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n","    params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n","    params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n","\n","    grad : 기울기를 보관하는 딕셔너리 변수(numerical_gradient()의 반환값)\n","    grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기.\n","    grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기.\n","    \"\"\"\n","    # 초기화를 수행한다.\n","    def __init__(self, input_size, hidden_size, output_size,\n","                 weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * \\\n","            np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * \\\n","            np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    # 예측(추론)을 수행한다.\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","\n","        return y\n","\n","    # 손실 함수의 값을 구한다.\n","    # x : 입력데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","\n","        return cross_entropy_error(y, t)\n","\n","    # 정확도를 구한다.\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    # 가중치 매개변수의 기울기를 구한다.\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads\n","\n","\n","if __name__ == '__main__':\n","    net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n","    print(net.params['W1'].shape)  # (784, 100)\n","    print(net.params['b1'].shape)  # (100,)\n","    print(net.params['W2'].shape)  # (100, 10)\n","    print(net.params['b2'].shape)  # (10,)\n","\n","    x = np.random.rand(100, 784)  # 더미 입력 데이터(100장 분량)\n","    t = np.random.rand(100, 10)   # 더미 정답 레이블(100장 분량)\n","\n","    grads = net.numerical_gradient(x, t)  # 기울기 계산\n","    # 주의 : 실행하는데 아주 오래걸림\n","    print(grads['W1'].shape)  # (784, 100)\n","    print(grads['b1'].shape)  # (100,)\n","    print(grads['W2'].shape)  # (100, 10)\n","    print(grads['b2'].shape)  # (10,)\n"]},{"cell_type":"markdown","metadata":{"id":"4-de-2sKF7lJ"},"source":["# 오차역전파법"]},{"cell_type":"markdown","metadata":{"id":"MlKAMjA1F_ME"},"source":["단순 계층 구현: 곱셈, 덧셈"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1666669520347,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"m0GQG6u8F-wk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27fd7897-06e5-4cf1-c76e-5a089e035c11"},"outputs":[{"output_type":"stream","name":"stdout","text":["220.00000000000003\n","2.2 110.00000000000001 200\n","715.0000000000001\n","110.00000000000001 2.2 3.3000000000000003 165.0 650\n"]}],"source":["# 5.4.1 곱셈 계층\n","class MulLayer:\n","    def __init__(self):\n","        self.x = None\n","        self.y = None\n","\n","    def forward(self, x, y):\n","        self.x = x\n","        self.y = y\n","        out = x * y\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * self.y\n","        dy = dout * self.x\n","        return dx, dy\n","\n","\n","# 5.4.2 덧셈 계층\n","class AddLayer:\n","    def __init__(self):\n","        pass\n","\n","    def forward(self, x, y):\n","        out = x + y\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * 1\n","        dy = dout * 1\n","        return dx, dy\n","\n","\n","if __name__ == '__main__':\n","    # 문제1의 예시\n","    apple = 100\n","    apple_num = 2\n","    tax = 1.1\n","\n","    # 계층들\n","    mul_apple_layer = MulLayer()\n","    mul_tax_layer = MulLayer()\n","\n","    # 순전파\n","    apple_price = mul_apple_layer.forward(apple, apple_num)\n","    price = mul_tax_layer.forward(apple_price, tax)\n","\n","    print(price)  # 220.0\n","\n","    # 역전파\n","    dprice = 1\n","    dapple_price, dtax = mul_tax_layer.backward(dprice)\n","    dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","    print(dapple, dapple_num, dtax)  # 2.2 110.0 200\n","\n","    # 문제2의 예시\n","    orange = 150\n","    orange_num = 3\n","\n","    # 계층들\n","    mul_apple_layer = MulLayer()\n","    mul_orange_layer = MulLayer()\n","    add_apple_orange_layer = AddLayer()\n","    mul_tax_layer = MulLayer()\n","\n","    # 순전파\n","    apple_price = mul_apple_layer.forward(apple, apple_num)\n","    orange_price = mul_orange_layer.forward(orange, orange_num)\n","    all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n","    price = mul_tax_layer.forward(all_price, tax)\n","\n","    print(price)  # 715.0\n","\n","    # 역전파\n","    dprice = 1\n","    dall_price, dtax = mul_tax_layer.backward(dprice)\n","    dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n","    dornage, dorange_num = mul_orange_layer.backward(dorange_price)\n","    dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n","\n","    print(dapple_num, dapple, dornage, dorange_num, dtax)\n","    # 110.0 2.2 3.3 165.0 650\n"]},{"cell_type":"markdown","metadata":{"id":"lJChMYmtGHJP"},"source":["활성화 함수, Affine, Softmax 계층 구현\n","- Affine Transformation: W*X + B"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1666669520348,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"_mdv96thGHZY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08524a65-bda6-4f9a-f73a-5adf69fa61ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1.  -0.5]\n"," [-2.   3. ]]\n","[[False  True]\n"," [ True False]]\n","(2,)\n","(2, 3)\n","(3,)\n","[[ 0  0  0]\n"," [10 10 10]]\n","[[ 1  2  3]\n"," [11 12 13]]\n","[[1 2 3]\n"," [4 5 6]]\n","[5 7 9]\n","0.007620616629495912\n","[0.00090496 0.65907491 0.00668679]\n","5.0076057626568575\n","[ 9.04959183e-04 -3.26646539e-01  9.92408247e-01]\n"]}],"source":["import numpy as np\n","\n","# 5.5.1 ReLU 계층\n","\"\"\"\n","y = x (x > 0)\n","    0 (x <= 0)\n","∂y/∂x  = 1 (x > 0)\n","         0 (x <= 0)\n","\n","ReLU의 계산 그래프\n","if x > 0\n","x     → relu → y\n","∂L/∂y ← relu ← ∂L/∂y\n","\n","if x <= 0\n","x → relu → y\n","0 ← relu ← ∂L/∂y\n","\"\"\"\n","\n","\n","class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","\n","# 5.5.2 Sigmoid 계층\n","\"\"\"\n","y = 1 / (1 + exp(-x))\n","\n","시그모이드의 계산 그래프\n","x → × → exp → + → / → y\n","-1↗         1↗\n","\n","1단계\n","'/'노드\n","y = 1/x\n","∂y/∂x = -1/x^2 = -y²\n","상류에서 흘러온 값에 -y^2(순전파의 출력을 제곱하고 마이너스)을 곱해서 하류로 전달 : -∂L/∂y*y²\n","\n","2단계\n","'+'노드\n","상류의 값을 그대로 하류로 전달 : -∂L/∂y*y²\n","\n","3단계\n","'exp'노드\n","y = exp(x)\n","∂y/∂x = exp(x)\n","상류의 값에 순전파 때의 출력(이 경우엔 exp(-x))을 곱해 하류로 전달 : -∂L/∂y*y²*exp(-x)\n","\n","4단계\n","'×'노드\n","순전파 때의 값을 서로 바꿔 곱함(여기서는 * -1) : ∂L/∂y*y²*exp(-x)\n","∂L/∂y*y^2*exp(-x)는 정리하면 ∂L/∂y*y(1-y)가 된다.(순전파의 출력만으로 계산할 수 있다)\n","\n","정리\n","x            → sigmoid → y\n","∂L/∂y*y(1-y) ← sigmoid ← ∂L/∂y\n","\"\"\"\n","\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = 1 / (1 + np.exp(-x))\n","        self.out = out\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","\n","\n","if __name__ == '__main__':\n","    x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n","    print(x)\n","    \"\"\"\n","    [[ 1.  -0.5]\n","     [-2.   3. ]]\n","    \"\"\"\n","\n","    mask = (x <= 0)\n","    print(mask)\n","    \"\"\"\n","    [[False  True]\n","     [ True False]]\n","    \"\"\"\n","\n","    # 5.6 Affine/Softmax 계층 구현하기\n","    # 5.6.1 Affine 계층\n","    \"\"\"\n","    신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 내적을 사용했다.(3.3 참고)\n","    \"\"\"\n","    X = np.random.rand(2)     # 입력\n","    W = np.random.rand(2, 3)  # 가중치\n","    B = np.random.rand(3)     # 편향\n","\n","    print(X.shape)  # (2,)\n","    print(W.shape)  # (2, 3)\n","    print(B.shape)  # (3,)\n","\n","    Y = np.dot(X, W) + B\n","    # 신경망의 순전파 때 수행하는 행렬의 내적은 기하학에서는 어파인 변환이라고 한다.\n","\n","\"\"\"\n","Affine 계층의 계산 그래프\n","X, W, B는 행렬(다차원 배열)\n","\n","1. X ↘    X·W\n","       dot → + → Y\n","2. W ↗ 3.B ↗\n","\n","1. ∂L/∂X = ∂L/∂Y·W^T\n","   (2,)    (3,)  (3,2)\n","2. ∂L/∂W = X^T·∂L/∂Y\n","   (2,3)  (2,1)(1,3)\n","3. ∂L/∂B = ∂L/∂Y\n","   (3,)    (3,)\n","W^T : W의 전치행렬(W가 (2,3)이라면 W^T는(3,2)가 된다.)\n","X = (x0, x1, x2, ..., xn)\n","∂L/∂X = (∂L/∂x0, ∂L/∂x1, ∂L/∂x2, ..., ∂L/∂xn)\n","따라서 X와 ∂L/∂X의 형상은 같다.\n","\"\"\"\n","\n","# 5.6.2 배치용 Affine 계층\n","\"\"\"\n","입력 데이터로 X 하나만이 아니라 데이터 N개를 묶어 순전파하는 배치용 계층을 생각\n","\n","배치용 Affine 계층의 계산 그래프\n","X의 형상이 (N,2)가 됨.\n","\n","1. ∂L/∂X = ∂L/∂Y·W^T\n","   (N,2)   (N,3) (3,2)\n","2. ∂L/∂W = X^T·∂L/∂Y\n","   (2,3)  (2,N)(N,3)\n","3. ∂L/∂B = ∂L/∂Y의 첫 번째 축(0축, 열방향)의 합.\n","   (3,)    (N,3)\n","\n","편향을 더할 때에 주의해야 한다. 순전파 때의 편향 덧셈은 X·W에 대한 편향이\n","각 데이터에 더해진다. 예를 들어 N=2일 경우 편향은 두 데이터 각각에 더해진다.\n","\"\"\"\n","if __name__ == '__main__':\n","    X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n","    B = np.array([1, 2, 3])\n","    print(X_dot_W)\n","    \"\"\"\n","    [[ 0  0  0]\n","     [10 10 10]]\n","    \"\"\"\n","    print(X_dot_W + B)\n","    \"\"\"\n","    [[ 1  2  3]\n","     [11 12 13]]\n","    \"\"\"\n","    \"\"\"\n","    순전파의 편향 덧셈은 각각의 데이터에 더해지므로\n","    역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.\n","    \"\"\"\n","    dY = np.array([[1, 2, 3], [4, 5, 6]])\n","    print(dY)\n","    \"\"\"\n","    [[1 2 3]\n","     [4 5 6]]\n","    \"\"\"\n","    dB = np.sum(dY, axis=0)\n","    print(dB)  # [5 7 9]\n","    \"\"\"\n","    데이터가 두 개일 때 편향의 역전파는 두 데이터에 대한 미분을 데이터마다\n","    더해서 구한다.\n","    np.sum()에서 0번째 축(데이터를 단위로 한 축. axis=0)에 대해서 합을 구한다.\n","    \"\"\"\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        self.x = None\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        self.x = x\n","        out = np.dot(x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","\n","        return dx\n","\n","\n","# 5.6.3 Softmax-with-Loss 계층\n","\"\"\"\n","소프트맥스 계층 : 입력 값을 정규화(출력의 합이 1이 되도록 변경)하여 출력\n","학습과 추론 중 학습에서 주로 사용\n","소프트맥스 계층과 손실 함수(교차 엔트로피 오차)를 포함해 계산 그래프를 그림\n","자세한 역전파 계산은 부록A 참고.\n","\n","간소화한 Softmax-with-Loss계층의 계산 그래프\n","a1   →    |         | → y1 → |         |\n","y1 - t1 ← |         |   t1 ↗  | Cross   |\n","a2   →    | Softmax | → y2 → | Entropy | → L\n","y2 - t2 ← |         |   t2 ↗  | Error   | ← 1\n","a3   →    |         | → y3 → |         |\n","y3 - t3 ←               t3 ↗\n","입력 : (a1, a2, a3)\n","정규화된 출력 : (y1, y2, y3)\n","정답 레이블 (t1, t2, t3)\n","손실 : L\n","\n","역전파로 Softmax 계층의 출력과 정답 레이블의 차분 값\n","(y1 - t1, y2 - t2, y2 - t2)이 전달됨.\n","이는 교차 엔트로피 오차 함수가 그렇게 설계되었기 때문.\n","항등 함수의 손실 함수로는 평균 제곱 오차를 사용하는데,\n","그럴 경우 역전파의 결과가 (y1 - t1, y2 - t2, y2 - t2)로 말끔히 떨어짐.\n","\n","ex) 정답 레이블 t = (0, 1, 0) 일 때,\n","소프트맥스가 (0.3, 0.2, 0.5)를 출력했다고 할 때, 소프트맥스 계층의 역전파는\n","(0.3, -0.8, 0.5)로 앞 계층에 큰 오차를 전파하게 됨\n","소프트맥스가 (0.01, 0.99, 0.)을 출력했다면 역전파는 (0.01, -0.01, 0)\n","으로 오차가 작아짐\n","\"\"\"\n","\n","\n","# yk = exp(ak) / ∑(i=1 to n)(exp(ai))\n","def softmax(a):\n","    c = np.max(a)\n","    exp_a = np.exp(a - c)  # 오버플로 대책\n","    sum_exp_a = np.sum(exp_a)\n","    y = exp_a / sum_exp_a\n","\n","    return y\n","\n","\n","def cross_entropy_error(y, t):\n","    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n","    return -np.sum(t * np.log(y + delta))\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None  # 손실\n","        self.y = None     # softmax의 출력\n","        self.t = None     # 정답 레이블(원-핫 벡터)\n","\n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)  # 3.5.2, 4.2.2에서 구현\n","        self.loss = cross_entropy_error(self.y, self.t)\n","\n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        dx = self.y - self.t / batch_size\n","\n","        return dx\n","\n","\n","if __name__ == '__main__':\n","    swl = SoftmaxWithLoss()\n","    a = np.array([1, 8, 3])   # 비슷하게 맞춤\n","    t = np.array([0, 1, 0])\n","    print(swl.forward(a, t))  # 0.0076206166295\n","    print(swl.backward())     # [ 0.00090496  0.65907491  0.00668679]\n","\n","    a = np.array([1, 3, 8])   # 오차가 큼\n","    print(swl.forward(a, t))  # 5.00760576266\n","    print(swl.backward())   # [  9.04959183e-04 -3.26646539e-01 9.92408247e-01]\n"]},{"cell_type":"markdown","metadata":{"id":"ch5cu_brGP8M"},"source":["오차역전파법 구현"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":10409,"status":"ok","timestamp":1666669530750,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"QU1OaSjkGPwV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f9ef78c-0699-42f4-f15e-f83dfe3c3d4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["W1:3.02721825537068e-13\n","b1:1.062745827755049e-12\n","W2:9.697626222014699e-13\n","b2:1.1990409082285326e-10\n"]}],"source":["import os\n","import numpy as np\n","from collections import OrderedDict\n","from common.layers import *\n","from common.gradient import numerical_gradient\n","from dataset.mnist import load_mnist\n","\n","# 5.7.1 신경망 학습의 전체 그림\n","\"\"\"\n","(4.5와 동일)\n","전제\n","신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n","신경망 학습은 다음과 같이 4단계로 수행한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n","그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n","\n","2단계 - 기울기 산출\n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신\n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복한다.\n","\n","수치 미분과 오차역전파법은 2단계에서 사용\n","수치 미분은 구현은 쉽지만 계산이 오래걸림\n","오차역전파법을 통해 기울기를 효율적이고 빠르게 구할 수 있음\n","\"\"\"\n","\n","# 5.7.2 오차역전파법을 이용한 신경망 구현하기\n","\"\"\"\n","TwoLayerNet 클래스로 구현\n"," * 클래스의 인스턴스 변수\n","params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n","        params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n","        params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n","layers : 신경망의 계층을 보관하는 순서가 있는 딕셔너리 변수\n","        layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이\n","        각 계층을 순서대로 유지\n","lastLayer : 신경망의 마지막 계층(여기서는 SoftmaxWithLoss)\n","\n"," * 클래스의 메서드\n","__init__(...) : 초기화 수행\n","predict(x) : 예측(추론)을 수행한다. x는 이미지 데이터\n","loss(x, t) : 손실함수의 값을 구한다. x는 이미지 데이터, t는 정답 레이블\n","accuracy(x, t) : 정확도를 구한다.\n","numerical_gradient(x, t) : 가중치 매개변수의 기울기를 수치 미분으로 구함(앞 장과 같음)\n","gradient(x, t) : 가중치 매개변수의 기울기를 오차역전파법으로 구함\n","\"\"\"\n","\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size,\n","        weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","        self.lastLayer = SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1:\n","            t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        # 순전파\n","        self.loss(x, t)\n","\n","        # 역전파\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'] = self.layers['Affine1'].dW\n","        grads['b1'] = self.layers['Affine1'].db\n","        grads['W2'] = self.layers['Affine2'].dW\n","        grads['b2'] = self.layers['Affine2'].db\n","\n","        return grads\n","\n","\n","\"\"\"\n","신경망의 계층을 순서가 있는 딕셔너리에서 보관,\n","따라서 순전파때는 추가한 순서대로 각 계층의 forward()를 호출하기만 하면 된다.\n","역전파때는 계층을 반대 순서로 호출하기만 하면 된다.\n","신경망의 구성 요소를 모듈화하여 계층으로 구현했기 때문에 구축이 쉬워진다.\n","\"\"\"\n","\n","\n","# 5.7.3 오차역전파법으로 구한 기울기 검증하기\n","\"\"\"\n","기울기를 구하는데는 두 가지 방법이 있다.\n","1. 수치 미분 : 느리다. 구현이 쉽다.\n","2. 해석적으로 수식을 풀기(오차 역전파법) : 빠르지만 실수가 있을 수 있다.\n","두 기울기 결과를 비교해서 오차역전파법을 제대로 구현했는지 검증한다.\n","이 작업을 기울기 확인gradient check라고 한다.\n","\"\"\"\n","if __name__ == '__main__':\n","    # 데이터 읽기\n","    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","    network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","    x_batch = x_train[:3]\n","    t_batch = t_train[:3]\n","\n","    grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","    grad_backprop = network.gradient(x_batch, t_batch)\n","\n","    # 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n","    for key in grad_numerical.keys():\n","        diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","        print(key + \":\" + str(diff))\n","        \"\"\"\n","        수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다.\n","        실수 없이 구현되었을 확률이 높다.\n","        정밀도가 유한하기 때문에 오차가 0이 되지는 않는다.\n","        \"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"LUez16mqGXdp"},"source":["오차역전파법을 사용한 학습 구현"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":34727,"status":"ok","timestamp":1666669565472,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"},"user_tz":-540},"id":"R5XLKFKAGXuh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a75338ab-52e2-4ac3-be81-aeda65ed6529"},"outputs":[{"output_type":"stream","name":"stdout","text":["train acc, test acc | 0.109, 0.1136\n","train acc, test acc | 0.9051333333333333, 0.9069\n","train acc, test acc | 0.92165, 0.9212\n","train acc, test acc | 0.9334166666666667, 0.9336\n","train acc, test acc | 0.9450166666666666, 0.943\n","train acc, test acc | 0.9513333333333334, 0.9482\n","train acc, test acc | 0.9566166666666667, 0.9532\n","train acc, test acc | 0.9614166666666667, 0.9572\n","train acc, test acc | 0.9634166666666667, 0.9582\n","train acc, test acc | 0.9661166666666666, 0.9613\n","train acc, test acc | 0.9697333333333333, 0.9627\n","train acc, test acc | 0.9711, 0.9635\n","train acc, test acc | 0.9733, 0.9669\n","train acc, test acc | 0.9743833333333334, 0.9662\n","train acc, test acc | 0.9755, 0.9684\n","train acc, test acc | 0.97745, 0.9689\n","train acc, test acc | 0.9785833333333334, 0.9699\n","## best test acc | 0.9699\n"]}],"source":["import os\n","import numpy as np\n","from dataset.mnist import load_mnist\n","\n","# 하이퍼 파라미터\n","iters_num = 10000  # 반복횟수\n","train_size = x_train.shape[0]\n","batch_size = 100  # 미니배치 크기\n","learning_rate = 0.1\n","seed = 0  # 실험결과 재현을 위함, 수정 불가\n","seed_everything(seed)\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","best_test_acc = 0\n","for i in range(iters_num):\n","    # print(i)\n","    # 미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 오차역전파법으로 기울기 계산\n","    grad = network.gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1에폭 당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n","        if test_acc > best_test_acc:\n","            best_test_acc = max(test_acc, best_test_acc)\n","\n","print(\"## best test acc | \" + str(best_test_acc))\n"]},{"cell_type":"markdown","metadata":{"id":"nRP04ogeUJkm"},"source":["# 실습 과제: 개선시도 2가지, 0.9699 보다 높은 test accuracy 달성\n","아래의 2가지를 모두 개선 시도하여 0.9699 보다 높은 test accuracy 달성하시오. 단, PyTorch, 학습된 모델, CNN 사용 불가합니다.\n","\n","* 1) 네트워크 구조 개선(아래의 TwoLayerNet 계층 수정): Affine 최소 3개 이상, ReLU 최소 2개 이상 사용\n"," * Affine 개수에 맞게 가중치 초기화, gradient 계산, 매개변수 갱신 부분도 수정할 것\n","  * 개선내용 설명: (Affine 3개, ReLU 2개 사용했습니다)\n","* 2) 학습 하이퍼파라미터 변경\n"," * 개선내용 설명: (iters_num = 50000으로 변경해보았습니다)"]},{"cell_type":"markdown","metadata":{"id":"qDOdPYaCIba6"},"source":["오차역전파법 구현"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"gqMjIFdHIba6","executionInfo":{"status":"ok","timestamp":1666669575373,"user_tz":-540,"elapsed":9905,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4a370f08-287c-4a62-cc04-aacea3c60fcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["W1:2.519068139686189e-13\n","b1:1.0554086299569346e-12\n","W2:6.198164914472605e-07\n","b2:8.243809981956418e-05\n","W3:8.365920258355887e-13\n","b3:1.2034818003270332e-10\n"]}],"source":["import os\n","import numpy as np\n","from collections import OrderedDict\n","from common.layers import *\n","from common.gradient import numerical_gradient\n","from dataset.mnist import load_mnist\n","\n","# 5.7.1 신경망 학습의 전체 그림\n","\"\"\"\n","(4.5와 동일)\n","전제\n","신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.\n","신경망 학습은 다음과 같이 4단계로 수행한다.\n","\n","1단계 - 미니배치\n","훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n","그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n","\n","2단계 - 기울기 산출\n","미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n","기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n","\n","3단계 - 매개변수 갱신\n","가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n","\n","4단계 - 반복\n","1~3단계를 반복한다.\n","\n","수치 미분과 오차역전파법은 2단계에서 사용\n","수치 미분은 구현은 쉽지만 계산이 오래걸림\n","오차역전파법을 통해 기울기를 효율적이고 빠르게 구할 수 있음\n","\"\"\"\n","\n","# 5.7.2 오차역전파법을 이용한 신경망 구현하기\n","\"\"\"\n","TwoLayerNet 클래스로 구현\n"," * 클래스의 인스턴스 변수\n","params : 신경망의 매개변수를 보관하는 딕셔너리 변수.\n","        params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향.\n","        params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향.\n","layers : 신경망의 계층을 보관하는 순서가 있는 딕셔너리 변수\n","        layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이\n","        각 계층을 순서대로 유지\n","lastLayer : 신경망의 마지막 계층(여기서는 SoftmaxWithLoss)\n","\n"," * 클래스의 메서드\n","__init__(...) : 초기화 수행\n","predict(x) : 예측(추론)을 수행한다. x는 이미지 데이터\n","loss(x, t) : 손실함수의 값을 구한다. x는 이미지 데이터, t는 정답 레이블\n","accuracy(x, t) : 정확도를 구한다.\n","numerical_gradient(x, t) : 가중치 매개변수의 기울기를 수치 미분으로 구함(앞 장과 같음)\n","gradient(x, t) : 가중치 매개변수의 기울기를 오차역전파법으로 구함\n","\"\"\"\n","\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size,\n","        weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, hidden_size)\n","        self.params['b2'] = np.zeros(hidden_size)\n","        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b3'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","        self.layers['Relu2'] = Relu() \n","        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n","        self.lastLayer = SoftmaxWithLoss()\n","\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","\n","        return x\n","\n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1:\n","            t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n","        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n","\n","        return grads\n","\n","    def gradient(self, x, t):\n","        # 순전파\n","        self.loss(x, t)\n","\n","        # 역전파\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","\n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'] = self.layers['Affine1'].dW\n","        grads['b1'] = self.layers['Affine1'].db\n","        grads['W2'] = self.layers['Affine2'].dW\n","        grads['b2'] = self.layers['Affine2'].db\n","        grads['W3'] = self.layers['Affine3'].dW\n","        grads['b3'] = self.layers['Affine3'].db\n","\n","        return grads\n","\n","\n","\"\"\"\n","신경망의 계층을 순서가 있는 딕셔너리에서 보관,\n","따라서 순전파때는 추가한 순서대로 각 계층의 forward()를 호출하기만 하면 된다.\n","역전파때는 계층을 반대 순서로 호출하기만 하면 된다.\n","신경망의 구성 요소를 모듈화하여 계층으로 구현했기 때문에 구축이 쉬워진다.\n","\"\"\"\n","\n","\n","# 5.7.3 오차역전파법으로 구한 기울기 검증하기\n","\"\"\"\n","기울기를 구하는데는 두 가지 방법이 있다.\n","1. 수치 미분 : 느리다. 구현이 쉽다.\n","2. 해석적으로 수식을 풀기(오차 역전파법) : 빠르지만 실수가 있을 수 있다.\n","두 기울기 결과를 비교해서 오차역전파법을 제대로 구현했는지 검증한다.\n","이 작업을 기울기 확인gradient check라고 한다.\n","\"\"\"\n","if __name__ == '__main__':\n","    # 데이터 읽기\n","    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","\n","    network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","    x_batch = x_train[:3]\n","    t_batch = t_train[:3]\n","\n","    grad_numerical = network.numerical_gradient(x_batch, t_batch)\n","    grad_backprop = network.gradient(x_batch, t_batch)\n","\n","    # 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.\n","    for key in grad_numerical.keys():\n","        diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n","        print(key + \":\" + str(diff))\n","        \"\"\"\n","        수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다.\n","        실수 없이 구현되었을 확률이 높다.\n","        정밀도가 유한하기 때문에 오차가 0이 되지는 않는다.\n","        \"\"\"\n"]},{"cell_type":"markdown","metadata":{"id":"p-29NkLzIba7"},"source":["오차역전파법을 사용한 학습 구현"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"n_wSz0YuIba7","executionInfo":{"status":"ok","timestamp":1666669862782,"user_tz":-540,"elapsed":204066,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b7293ed1-6787-402b-a9b0-ad37cd28c81c"},"outputs":[{"output_type":"stream","name":"stdout","text":["train acc, test acc | 0.08626666666666667, 0.0834\n","train acc, test acc | 0.6481, 0.649\n","train acc, test acc | 0.8866166666666667, 0.8858\n","train acc, test acc | 0.9253333333333333, 0.9268\n","train acc, test acc | 0.9464, 0.9425\n","train acc, test acc | 0.9561166666666666, 0.9501\n","train acc, test acc | 0.9641166666666666, 0.9594\n","train acc, test acc | 0.9674333333333334, 0.9622\n","train acc, test acc | 0.9695333333333334, 0.9638\n","train acc, test acc | 0.9734333333333334, 0.9674\n","train acc, test acc | 0.9744833333333334, 0.9656\n","train acc, test acc | 0.9754833333333334, 0.9646\n","train acc, test acc | 0.9790166666666666, 0.9683\n","train acc, test acc | 0.9797833333333333, 0.9686\n","train acc, test acc | 0.9811666666666666, 0.9684\n","train acc, test acc | 0.9842, 0.9718\n","train acc, test acc | 0.9849833333333333, 0.9696\n","train acc, test acc | 0.9864666666666667, 0.9722\n","train acc, test acc | 0.9868333333333333, 0.9734\n","train acc, test acc | 0.9864833333333334, 0.9719\n","train acc, test acc | 0.9850333333333333, 0.9699\n","train acc, test acc | 0.98955, 0.9735\n","train acc, test acc | 0.9891, 0.9731\n","train acc, test acc | 0.9884666666666667, 0.9716\n","train acc, test acc | 0.9907333333333334, 0.974\n","train acc, test acc | 0.9920166666666667, 0.9742\n","train acc, test acc | 0.9923833333333333, 0.9736\n","train acc, test acc | 0.9912833333333333, 0.9718\n","train acc, test acc | 0.99305, 0.9718\n","train acc, test acc | 0.9939166666666667, 0.9729\n","train acc, test acc | 0.9944333333333333, 0.9741\n","train acc, test acc | 0.9939166666666667, 0.9729\n","train acc, test acc | 0.991, 0.9678\n","train acc, test acc | 0.9957, 0.9746\n","train acc, test acc | 0.9946333333333334, 0.9713\n","train acc, test acc | 0.9957166666666667, 0.9729\n","train acc, test acc | 0.9960333333333333, 0.9746\n","train acc, test acc | 0.99685, 0.9736\n","train acc, test acc | 0.9976833333333334, 0.9749\n","train acc, test acc | 0.9965333333333334, 0.9718\n","train acc, test acc | 0.9965166666666667, 0.9715\n","train acc, test acc | 0.99795, 0.9742\n","train acc, test acc | 0.99805, 0.9742\n","train acc, test acc | 0.9972333333333333, 0.9719\n","train acc, test acc | 0.9987333333333334, 0.9734\n","train acc, test acc | 0.9986, 0.973\n","train acc, test acc | 0.9986, 0.9716\n","train acc, test acc | 0.9992666666666666, 0.9745\n","train acc, test acc | 0.99915, 0.9736\n","train acc, test acc | 0.9994333333333333, 0.974\n","train acc, test acc | 0.9983166666666666, 0.9724\n","train acc, test acc | 0.9986833333333334, 0.9729\n","train acc, test acc | 0.9997, 0.974\n","train acc, test acc | 0.9997333333333334, 0.9738\n","train acc, test acc | 0.9997666666666667, 0.9746\n","train acc, test acc | 0.99975, 0.974\n","train acc, test acc | 0.9998, 0.9743\n","train acc, test acc | 0.9998833333333333, 0.974\n","train acc, test acc | 0.9998166666666667, 0.9739\n","train acc, test acc | 0.9999666666666667, 0.9735\n","train acc, test acc | 0.9999333333333333, 0.974\n","train acc, test acc | 0.9999666666666667, 0.9739\n","train acc, test acc | 0.9999833333333333, 0.9732\n","train acc, test acc | 0.9999833333333333, 0.9735\n","train acc, test acc | 0.9999833333333333, 0.9736\n","train acc, test acc | 1.0, 0.9737\n","train acc, test acc | 0.9999833333333333, 0.9732\n","train acc, test acc | 1.0, 0.9738\n","train acc, test acc | 1.0, 0.9741\n","train acc, test acc | 1.0, 0.9733\n","train acc, test acc | 0.9999833333333333, 0.9735\n","train acc, test acc | 1.0, 0.9733\n","train acc, test acc | 1.0, 0.9739\n","train acc, test acc | 0.9999833333333333, 0.9741\n","train acc, test acc | 1.0, 0.9741\n","train acc, test acc | 1.0, 0.9743\n","train acc, test acc | 1.0, 0.974\n","train acc, test acc | 1.0, 0.9731\n","train acc, test acc | 1.0, 0.9747\n","train acc, test acc | 1.0, 0.9739\n","train acc, test acc | 1.0, 0.9741\n","train acc, test acc | 1.0, 0.9735\n","train acc, test acc | 1.0, 0.9743\n","train acc, test acc | 1.0, 0.974\n","## best test acc | 0.9749\n"]}],"source":["import os\n","import numpy as np\n","from dataset.mnist import load_mnist\n","\n","# 하이퍼 파라미터\n","iters_num = 50000  # 반복횟수\n","train_size = x_train.shape[0]\n","batch_size = 100  # 미니배치 크기\n","learning_rate = 0.1\n","seed = 0  # 실험결과 재현을 위함, 수정 불가\n","seed_everything(seed)\n","\n","# 데이터 읽기\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","# 1에폭당 반복 수\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","best_test_acc = 0\n","for i in range(iters_num):\n","    # print(i)\n","    # 미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 오차역전파법으로 기울기 계산\n","    grad = network.gradient(x_batch, t_batch)\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2','W3','b3'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1에폭 당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n","        if test_acc > best_test_acc:\n","            best_test_acc = max(test_acc, best_test_acc)\n","\n","print(\"## best test acc | \" + str(best_test_acc))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Sw8EUyMqQqKn","executionInfo":{"status":"ok","timestamp":1666669614623,"user_tz":-540,"elapsed":475,"user":{"displayName":"안희진컴퓨터공학과","userId":"09104803564907682684"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1jIM1WShg-9uSQ8dtIuv0OMiOlgtB43ww","timestamp":1666138825939}]},"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}